\documentclass[10pt]{article}
\usepackage[hmargin=2cm, vmargin=3cm]{geometry}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{enumerate}
\usepackage{bbm}
\lstset{numbers=left,  
stepnumber=1,
literate={~} {$\sim$}{1}
}

\title{MATH 423 - Assignment 4}
\author{Yi Tian Xu\\260520039}
\date{November 26, 2014}


\begin{document}
\maketitle

\begin{enumerate}
	\item \begin{enumerate}[(a)]
		\item We fit the model 
		\begin{displaymath}
			Y_i = \beta_0 + \sum_{j=1}^5 \beta_{1j}^C \mathbbm{1}_j(x_{i1}) +\epsilon_i
		\end{displaymath}
\begin{lstlisting}
> data1 <- read.csv('data_ass4.csv')
> data1$X1 <- as.factor(data1$X1)
> fit<-lm(Y~X1,data=data1)
> summary(fit)

Call:
lm(formula = Y ~ X1, data = data1)

Residuals:
     Min       1Q   Median       3Q      Max 
-16.6667  -5.5833   0.1667   3.2500  24.0000 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   79.333      7.362  10.776 1.59e-07 ***
X12           52.333     10.412   5.026 0.000296 ***
X13           65.667     10.412   6.307 3.91e-05 ***
X14           80.667     10.412   7.748 5.21e-06 ***
X15          105.667     10.412  10.149 3.05e-07 ***
X16          113.667     10.412  10.917 1.38e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 12.75 on 12 degrees of freedom
Multiple R-squared:  0.9291,    Adjusted R-squared:  0.8995 
F-statistic: 31.45 on 5 and 12 DF,  p-value: 1.696e-06
		\end{lstlisting}
		\begin{enumerate}[(i)]
			\item At line 26, the F value is 31.45 with 5 and 12 degrees of freedom. The p-value is $1.696\times 10^{-6} < 0.01$. So at the $1\%$ level, we can reject the null hypothesis, i.e.: reject that there is no difference across the varieties of wheat.
			
			\item In this experiment, there are exactly 3 observations per variety of wheat. So we have a balanced design. 
			\begin{align*}
				X &= \left( \begin{array}{cccccc}
x_{10} & x_{11} & x_{12} & x_{13} & x_{14} & x_{15}\\
x_{20} & x_{21} & x_{22} & x_{23} & x_{24} & x_{25}\\
\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
x_{n0} & x_{n1} & x_{n2} & x_{n3} & x_{n4} & x_{n5} \end{array} \right)\\
		&=\left( \begin{array}{cccccc}
1&0&0&0&0&0\\
1&0&0&0&0&0\\
1&0&0&0&0&0\\
1&1&0&0&0&0\\
1&1&0&0&0&0\\
1&1&0&0&0&0\\
1&0&1&0&0&0\\
1&0&1&0&0&0\\
1&0&1&0&0&0\\
1&0&0&1&0&0\\
1&0&0&1&0&0\\
1&0&0&1&0&0\\
1&0&0&0&1&0\\
1&0&0&0&1&0\\
1&0&0&0&1&0\\
1&0&0&0&0&1\\
1&0&0&0&0&1\\
1&0&0&0&0&1 \end{array} \right)
			\end{align*}
			As $\beta_0$ is estimated using the $x_{i0}$'s, the first column is all 1's. Then, since $\mbox{e.s.e}(\hat{\beta}_j) = \sqrt{\sigma^2 V_{jj}}$, where $V_{jj}$ is the $j$th element on the diagonal of $(X^\top X)^{-1}$ (which is a $6\times 6$ matrix). In this case, all elements on the diagonal of $(X^\top X)^{-1}$ is $2/3$ except for the first one, which is $1/3$. So the $\mbox{e.s.e}(\hat{\beta}_j)$'s are equal $\forall j \in \{1,2,3,4,5\}$. 
		\end{enumerate}
		\item We fit all the models.
			\begin{lstlisting}
> data2 <- read.csv('http://www.math.mcgill.ca/dstephens/Regression/Data/Therapy.csv')
> data2$X1 <- as.factor(data2$X1)
> data2$X2 <- as.factor(data2$X2)
> 
> fit0<-lm(Y~1,data=data2)
> anova(fit0)
Analysis of Variance Table

Response: Y
           Df Sum Sq Mean Sq F value Pr(>F)
Residuals 175 4691.8   26.81               
> fit1<-lm(Y~X1,data=data2)
> anova(fit1)
Analysis of Variance Table

Response: Y
           Df Sum Sq Mean Sq F value  Pr(>F)  
X1          3  230.9  76.968  2.9677 0.03348 *
Residuals 172 4460.9  25.935                  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> fit2<-lm(Y~X2,data=data2)
> anova(fit2)
Analysis of Variance Table

Response: Y
           Df Sum Sq Mean Sq F value   Pr(>F)   
X2          2  306.9 153.474  6.0552 0.002872 **
Residuals 173 4384.8  25.346                    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> fit3<-lm(Y~X1+X2,data=data2)
> anova(fit3)
Analysis of Variance Table

Response: Y
           Df Sum Sq Mean Sq F value    Pr(>F)    
X1          3  230.9  76.968  3.1834 0.0253412 *  
X2          2  350.6 175.294  7.2501 0.0009516 ***
Residuals 170 4110.3  24.178                      
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> fit4<-lm(Y~X1*X2,data=data2)
> anova(fit4)
Analysis of Variance Table

Response: Y
           Df Sum Sq Mean Sq F value   Pr(>F)   
X1          3  230.9  76.968  3.1479 0.026630 * 
X2          2  350.6 175.294  7.1693 0.001035 **
X1:X2       6  100.4  16.730  0.6842 0.662594   
Residuals 164 4009.9  24.451                    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
			\end{lstlisting}
			
		The $SS_{RES}$ value for model 0 is 4691.8 (line 11); for model 1 is 4460.9 (line 19); for model 2 is 4384.8 (line 29); for model 3 is 4110.3 (line 40); for model 4 is 4009.9 (line 52).
		\\\\
		In the ANOVA table for model 4, we see that, at line 51, the F-statistics associated to the interaction between $X_1$ and $X_2$ has a p-value of $0.662594 > 0.1$. This implies that the null hypothesis 
		\begin{displaymath}
			H_0: \beta_{12}^C = 0
		\end{displaymath}
		 cannot be rejected, i.e.: there is not enough evidence that adding this interaction improves the fit. 
		 \\\\
		 Now, we can test to see if dropping $X_1$ or $X_2$ changes the adequacy of the fit. 
		 \begin{lstlisting}
> drop1(fit3, test="F")
Single term deletions

Model:
Y ~ X1 + X2
       Df Sum of Sq    RSS    AIC F value    Pr(>F)    
<none>              4110.3 566.53                      
X1      3    274.54 4384.8 571.91  3.7850 0.0115970 *  
X2      2    350.59 4460.9 576.94  7.2501 0.0009516 ***
		 \end{lstlisting}
		 At line 8, we compare model 3 with model 2 and use F-statistics to test the null hypothesis that the intercept associated to $X_1$ is 0. This null hypothesis is rejected at the $5\%$ level (p-value is 0.0115970). 
		 At line 8, we compare model 3 with model 1 and use F-statistics to test the null hypothesis that the intercept associated to $X_2$ is 0. This null hypothesis is rejected at the $1\%$ level (p-value is 0.0009516). 
		 \\\\
Therefore, we can conclude that $X_1$ and $X_2$ has an effect on depression score individually. So they have main effects. 
	\end{enumerate}
	\item \begin{enumerate}[a)]
		\item The design is balanced. 
		\begin{lstlisting}
> data3 <- read.csv('http://www.math.mcgill.ca/dstephens/Regression/Data/Mobility.csv')
> data3$X1 <- as.factor(data3$X1)
> data3$X2 <- as.factor(data3$X2)
> 
> table(data3$X1,data3$X2)
   
     1  2
  1 40 40
  2 40 40
  3 40 40

		\end{lstlisting}
		We can see that there are exactly 40 subjects per factor level combination (lines 7-10). %Furthermore, we can also fit the models $X_1*X_2$ and $X_2*X_1$.
%	\begin{lstlisting}
%> fit0 <- lm(Y~X1*X2, data=data3)
%> anova(fit0)
%Analysis of Variance Table

%Response: Y
%           Df Sum Sq Mean Sq F value Pr(>F)
%X1          2    343 171.614  1.0155 0.3638
%X2          1     12  12.381  0.0733 0.7869
%X1:X2       2    326 162.806  0.9634 0.3831
%Residuals 234  39543 168.989               
%> 
%> fit1 <- lm(Y~X2*X1, data=data3)
%> anova(fit1)
%Analysis of Variance Table

%Response: Y
%           Df Sum Sq Mean Sq F value Pr(>F)
%X2          1     12  12.381  0.0733 0.7869
%X1          2    343 171.614  1.0155 0.3638
%X2:X1       2    326 162.806  0.9634 0.3831
%Residuals 234  39543 168.989 
  
%	\end{lstlisting}
	% We can see that the order of the factor predictors in the model does not change the sum of squares decomposition, or the assessment of statistical significance. That is, the ANOVA tables for the two fits are identical up to the order of the rows. 
		\item We start with the model $X_1*X_2*X_3*X_4$.
		\begin{lstlisting}
> fit2 <- lm(Y~X1*X2*X3*X4, data=data3)
> anova(fit2)
Analysis of Variance Table

Response: Y
             Df Sum Sq Mean Sq    F value  Pr(>F)    
X1            2    343     172    52.9875 < 2e-16 ***
X2            1     12      12     3.8228 0.05185 .  
X3            1  38262   38262 11813.7196 < 2e-16 ***
X4            1    859     859   265.2529 < 2e-16 ***
X1:X2         2      3       2     0.5298 0.58949    
X1:X3         2      4       2     0.6243 0.53660    
X2:X3         1      6       6     1.8867 0.17099    
X1:X4         2      1       0     0.1330 0.87554    
X2:X4         1      0       0     0.0981 0.75441    
X3:X4         1      0       0     0.1539 0.69518    
X1:X2:X3      2     23      12     3.6234 0.02833 *  
X1:X2:X4      2      3       1     0.4062 0.66666    
X1:X3:X4      2      3       1     0.4407 0.64415    
X2:X3:X4      1      2       2     0.5335 0.46592    
X1:X2:X3:X4   2      2       1     0.3748 0.68786    
Residuals   216    700       3                       
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
		\end{lstlisting}
		
		We see that the F-statistic associated to the interaction $X_1:X_2:X_3:X_4$ has p-value $0.68786 > 0.1$ (line 21). So we can drop that interaction. Furthermore, we see that the interactions involving $X_4$ all have an associated F-statistics with p-values that are high (lines 14-16, 18-21). Thus, perhaps we can remove all such interactions.  
		
		\begin{lstlisting}
> fit3 <- lm(Y~X1*X2*X3+X4, data=data3)
> anova(fit3, fit2)
Analysis of Variance Table

Model 1: Y ~ X1 * X2 * X3 + X4
Model 2: Y ~ X1 * X2 * X3 * X4
  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1    227 711.26                           
2    216 699.57 11    11.681 0.3279  0.979
		\end{lstlisting}
		
		We compare the models $X_1 * X_2 * X_3 + X_4$ and $X_1 * X_2 * X_3 * X_4$. The F-statistics for testing the null hypothesis 
		\begin{displaymath}
			H_0: \beta_{14j}^C = \beta_{24l}^C = \beta_{34} = \beta_{124jl}^C = \beta_{134j}^C = \beta_{234l}^C = \beta_{1234jl}^C = 0 \  \forall j \in \{1,2,3\}, \forall l \in \{1,2\}
		\end{displaymath}
		It has p-value $0.979$ (line 9) close to 1. This means that we cannot reject $H_0$. Thus, the model $X_1 * X_2 * X_3 + X_4$ is an adequate and simpler fit for the data.
		\begin{lstlisting}
> anova(fit3)
Analysis of Variance Table

Response: Y
           Df Sum Sq Mean Sq    F value  Pr(>F)    
X1          2    343     172    54.7714 < 2e-16 ***
X2          1     12      12     3.9515 0.04803 *  
X3          1  38262   38262 12211.4433 < 2e-16 ***
X4          1    859     859   274.1830 < 2e-16 ***
X1:X2       2      3       2     0.5476 0.57908    
X1:X3       2      4       2     0.6453 0.52545    
X2:X3       1      6       6     1.9503 0.16392    
X1:X2:X3    2     23      12     3.6877 0.02654 *  
Residuals 227    711       3                       
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
		\end{lstlisting}
		
		Here we see that the interactions $X_1:X_2$, $X_2:X_3$ and $X_1:X_3$ have associated F-statistics with large p-values (lines 10-12). Thus is it possible that we can drop these interactions despite that the F-statistics associated to the interaction $X_1:X_2:X_3$ has a small p-values ($0.02654 < 0.05$) (line 13).
		
		\begin{lstlisting}
> fit4 <- lm(Y~X1+X2+X3+X4, data=data3)
> anova(fit4, fit3)
Analysis of Variance Table

Model 1: Y ~ X1 + X2 + X3 + X4
Model 2: Y ~ X1 * X2 * X3 + X4
  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1    234 747.95                           
2    227 711.26  7    36.696 1.6731 0.1166
		\end{lstlisting}
		We compare the models $X_1 + X_2 + X_3 + X_4$ and $X_1 * X_2 * X_3 + X_4$. The F-statistics to test the null hypothesis
		\begin{displaymath}
			H_0: \beta_{12jl}^C = \beta_{13j}^C = \beta_{23l}^C = \beta_{123jl}^C = 0 \  \forall j \in \{1,2,3\}, \forall l \in \{1,2\}
		\end{displaymath}
		The p-value is $0.1166 > 0.1$ (line 9), so we cannot reject the null hypothesis at the $10\%$ level. So the model $X_1 + X_2 + X_3 + X_4$ is an adequate and simpler fit of the data.
		\begin{lstlisting}
> anova(fit4)
Analysis of Variance Table

Response: Y
           Df Sum Sq Mean Sq    F value  Pr(>F)    
X1          2    343     172    53.6903 < 2e-16 ***
X2          1     12      12     3.8735 0.05024 .  
X3          1  38262   38262 11970.4182 < 2e-16 ***
X4          1    859     859   268.7713 < 2e-16 ***
Residuals 234    748       3                       
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
		\end{lstlisting}
		We see that the F-statistic associated with all the main effects are small (less than 0.1) (lines 6-9). Yet, we still can check if we can drop one of the predictors in $X_1 + X_2 + X_3 + X_4$. 
		\begin{lstlisting}
> drop1(fit4, test="F")
Single term deletions

Model:
Y ~ X1 + X2 + X3 + X4
       Df Sum of Sq     RSS    AIC  F value    Pr(>F)    
<none>                748.0 284.81                       
X1      2     443.6  1191.6 392.57   69.393 < 2.2e-16 ***
X2      1     332.4  1080.4 371.06  104.000 < 2.2e-16 ***
X3      1   12672.5 13420.4 975.74 3964.645 < 2.2e-16 ***
X4      1     859.1  1607.0 466.36  268.771 < 2.2e-16 ***
		\end{lstlisting}
		We see that the F-statistics for comparing the model $X_1 + X_2 + X_3 + X_4$ with simpler models ($X_2 + X_3 + X_4$, $X_1 + X_3 + X_4$, $X_1 + X_2 + X_4$ and $X_1 + X_2 + X_3$) all have low p-values (less than 0.10). This suggests that dropping any predictor is likely to cause oversimplification.
		
		Thus, using backward elimination, we find that the model $X_1 + X_2 + X_3 + X_4$ is the simplest to explain the outcome adequately. 
		
		\begin{figure}[ht!]
		\center
		\includegraphics[width=80mm]{ass_4_res1.jpg}
		\includegraphics[width=80mm]{ass_4_res2.jpg}
		\end{figure}
		
		\begin{lstlisting}
> res <- resid(fit4) 
> plot(data3$X3,res,xlab="X3",ylab="residual", main="Residual vs X3")
> abline(h=0, lty="dotted")
> plot(data3$X4,res,xlab="X4",ylab="residual", main="Residual vs X4")
> abline(h=0, lty="dotted")
> summary(fit4)

Call:
lm(formula = Y ~ X1 + X2 + X3 + X4, data = data3)

Residuals:
   Min     1Q Median     3Q    Max 
-4.861 -1.185 -0.050  1.210  4.733 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 41.43021    1.34332  30.842  < 2e-16 ***
X12          2.08625    0.28415   7.342 3.46e-12 ***
X13         -1.23708    0.28433  -4.351 2.03e-05 ***
X22          2.36074    0.23149  10.198  < 2e-16 ***
X3          -2.00215    0.03180 -62.965  < 2e-16 ***
X4          -1.02247    0.06237 -16.394  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.788 on 234 degrees of freedom
Multiple R-squared:  0.9814,    Adjusted R-squared:  0.981 
F-statistic:  2470 on 5 and 234 DF,  p-value: < 2.2e-16

		\end{lstlisting}		
		
		Here, we can comment on the quality of the fit. We see that the p-value associated to each predictor is low (line 17-22). The coefficients for $X_3$ and $X_4$ are both negative, indicating significant negative relationship with the outcome. The $R^2$ and the adjusted $R^2$ are very close to 1, indicating that the model is a good fit for the data. 
		
		In both residual plots, we see that the  points are scattered with no evident pattern, and the variance of those points also seems constant. This also suggests that the model is a good fit for the data. 
		
		\item The hospital in which the surgery is performed ($X_1$) affects the mobility outcome as the simplest model found in previously to fit the data contains that factor. It has main effect. 
		%\\\\
		%In fact, this can be also shown by checking the F-statistics associated to comparing the model $X_1+X_2+X_3+X_4$ with $X_2+X_3+X_4$ at line 8 in the code for the previous question. 
		\item We can check if there is a correlation between $X_1$ and $X_3$ or $X_4$ by carrying out a one-way analysis of variance.
		\begin{lstlisting}
> anova(lm(X3~X1, data=data3))
Analysis of Variance Table

Response: X3
           Df Sum Sq Mean Sq F value Pr(>F)
X1          2   78.1  39.027  1.3648 0.2574
Residuals 237 6777.2  28.596               
> anova(lm(X4~X1, data=data3))
Analysis of Variance Table

Response: X4
           Df  Sum Sq Mean Sq F value Pr(>F)
X1          2   30.78 15.3915  2.0705 0.1284
Residuals 237 1761.77  7.4336      
		\end{lstlisting} 
		At line 6, we have F-statistics associated with the test that asserts whether $X_1$ is a good predictor of $X_3$. It has a p-value of $0.2574 > 0.1$. Thus we can conclude that there is not enough evidence to say that there is a significance difference between the men recruited in terms of age across
the three hospitals.

	At line 13, we have F-statistics associated with the test that asserts whether $X_1$ is a good predictor of $X_4$. It has a p-value of $0.1284 > 0.1$. Thus we can conclude that there is not enough evidence to say that there is a significance difference between the men recruited in terms of BMI across
the three hospitals. 

		\item We can check if there's is a correlation between $X_3$ and $X_4$. We fit the model 
		\begin{displaymath}
			X_{i3} = \beta_0 + \beta_4X_{i4} + \epsilon
		\end{displaymath}
		\begin{lstlisting}
> fit6<-summary(lm(X3~X4, data=data3))
> fit6

Call:
lm(formula = X3 ~ X4, data = data3)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.7255 -2.5168 -0.3241  2.6872 10.9411 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -25.53621    2.15359  -11.86   <2e-16 ***
X4            1.42943    0.08651   16.52   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 3.663 on 238 degrees of freedom
Multiple R-squared:  0.5343,    Adjusted R-squared:  0.5323 
F-statistic: 273.1 on 1 and 238 DF,  p-value: < 2.2e-16

> 1/(1-as.numeric(fit6[8]))
[1] 2.147269

		\end{lstlisting}
		At line 19, we have the $R^2$ value, and we use that at line 22 to calculate the VIF. We obtain a VIF of $2.147269 < 5$. Thus we can conclude that multicollinearity between the two continuous predictors is low. 
		\item We predict the outcome for each $X_2 \in \{1,2\}$ with $X_1 = 2$, $X_3 = 62-50$, $X_4 = 27.3$. 
		\begin{lstlisting}
> new.df <- data.frame(X1=as.factor(c(2,2)), 
+  X2=as.factor(c(1,2)), 
+  X3=c(12,12), 
+  X4=c(27.3,27.3))
> predict(fit4, new.df)
        1         2 
-8.422712 -6.061976 
		\end{lstlisting}
		Line 7 list the predicted outcome for each type of surgery (labeled at line 6). 
	\end{enumerate}
\end{enumerate}
\end{document}