<!DOCTYPE HTML>
<HTML>
	<HEAD>
		<TITLE>
		 	COMP 417 project
		</TITLE>
		<style>
		body {
			margin: 5% 25%;
			font-size: 16pt;
		}
		p.abstract {
		    font-style: italic;
			margin-left: 30px;
		}

		h1, h2, h3 {
		    font-family: Arial, Helvetica, sans-serif;
		}
		figure{
			text-align: center;
		}
		span.author{
			font-size: 14pt;
		}
		</style>
		<head>
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		  // MathJax.Hub.Register.StartupHook("AsciiMath Jax Config",function () {
		  //   var AM = MathJax.InputJax.AsciiMath.AM;
		  //   AM.symbols.push(
		  //     {input:"mathbi", tag:"mstyle", atname:"mathvariant", atval:"bold-italic",
		  //      output:"mathbi", tex:null, ttype:AM.TOKEN.UNARY}
		  //   );
		  // });
		  MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
		</script>
		<script type="text/javascript"
		  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>

		<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
		<script>
			$("a[href='#top']").click(function() {
			  $("html, body").animate({ scrollTop: 0 }, "slow");
			  return false;
			});
		</script>
</head>
	</HEAD>
<BODY>
	<center>
		<H1>
			Study on Network Calibration
		</H1>
		<span class="author">by Yi Tian Xu <br>
		Undergraduate in Statistics and Computer Science <br>
		@McGill University, December 2014</span>
	</center>
	<P class="abstract">
		This project outlines the idea proposed by Dimitrios Makris, Tim Ellis and James Black through their paper "<a href="http://www.cim.mcgill.ca/~dudek/417/Papers/sensenet-markis-e-b.pdf" target="blank">Briging the Gaps between Camera</a>" published in 2004. This
		project was required work for the 
		<a href="http://www.cim.mcgill.ca/~dudek/417.html">
		Introduction to Robotics
		</a>
		taught by <a href="http://www.cim.mcgill.ca/~dudek">Gregory Dudek</a>.
	<P>

<h2>Content</h2>
<ol>
	<li><a href="#1">Introduction</a></li>
	<li><a href="#2">Theory</a></li>
	<li><a href="#3">Implementation</a></li>
	<li><a href="#4">Experiment</a></li>
	<li><a href="#5">Conclusion</a></li>
</ol>

<a name="1"></a><h2>Introduction</h2>
<p>To track objects in an environmnet partially monitored by cameras, an intelligent surveillance system need to know how to deal with targets transiting through the blind regions between two spatially adjacent cameras. Such system would be able to continuously track targets and to generate complete target activity records. </p>

<p>Past research has been done on single camera tracking and matching tracks in overlapping camera views. Some methods have been proposed to track single object through multiple camera views. Notably, Javed et al suggest to correspond the individial tracks of a target using feature matching by maximizing the posterior probability of the spatio-temporal and color apparance of the target. <a href="#r2">[2]</a> However, color is not guaranteed to be always reliable to match targets across multiple cameras.</p>

<p> An alternative approach is to find a model of the environment and of the camera connectivity, enabling the system to predict the bahavior its targets. This allows one camera to "handover" the track to the next camera as the target move across the two, and thus avoids the need of establishing any correspondence. This model can carry spacial (topology) and temporal (time taken for targets to move between views) information of the network.
</p>

<p>The topology of the camera can be determined by using calibrated cameras, whose transformation function of pixel coordinates into 3D coordinates is already defined. If the coordinate system in which calibration is done is common to all the cameras, a geometric analysis of each camera views would then allows spatial adjacency to be established. However, in a large network of cameras, it would be preferable if the system can calibrate itself automatically. Furthermore, a common coordinate system might not be necessary since establishing spatial adjacency requires simply the knowledge of the cameras position relative to each other.</p> 

<p>This project inverstigates on unsupervised learning method, "network calibration",proposed by Makris et al, that estabilishes links between camera views without any requirement of correspondence of target tracks and pre-calibration. This method trains itself on a dataset of observed target trajectories and infers the topology of the camera network based on statistics. <a href="#r1">[1]</a>
</p>
<a href='#top'>[top]</a>

<a name="2"></a><h2>Theory</h2>
<p>Each camera view has a set of zones describing where targets enter or leave. These entry/exit zones can be represented as nodes in a network. Each edge between two nodes denote an transition which is either visibile or invisible to the cameras. Figure 1 shows an exmples of such network with two zones. </p> 

<figure>
	<img src="graphmodel.png" alt="Tempo-topographical model">
	<figcaption><b>Figure 1</b> The network showing entry zone $i$ and exit zone $j$.</figcaption>
</figure> 

<p>The transition probability of a target from zone $i$ to zone $j$ is $\alpha_{ij} = \int \alpha_{ij}(\tau) d\tau$ where $\alpha_{ij}(\tau)$ for each $\tau$ is the probability of transiting within a time $\tau$. Any region outside of zone $i$ and $j$ is represented by a virtual node $k$. Targets may move from zone $i$ to somewhere else with probability $\alpha_{ik}$ and enter zone $j$ from somewhere else with rate $\pi_{j}$. The transitions filful

\begin{equation*}\sum_{Z} \alpha_{iZ} = 1\end{equation*}

A target disappearing at zone $i$ and reapearing at zone $j$ will kindle two signals to the cameras. These signals are modeled as Bernoulli processes, $N_i$ and $M_j$ respectively, and they are assumed to be indvidually and jointly stationary. That is, at any time $t$, the probability of seeing a target appearig at zone $i$ is 

\begin{equation*}p_n = p(n_i(t) = 1) = E[N_i]\end{equation*}

and the probability of seen a target dissapearing at zone $j$ is

\begin{equation*}p_m = p(m_j(t) = 1) = E[M_j]\end{equation*}

If zone $i$ and zone $j$ are adjacent or overlapping, then their associated signals are correlated, i.e.: if a target appears in zone $i$, there is a higher chance that it will appear in zone $j$ after some time $\tau^*$. This implies that the transition probability $\alpha_{ij}(\tau^*)$ is much higher than $\alpha_{ij}(\tau) \ \forall \tau \neq \tau^*$. Such links between zones can be detected by estimating the cross-correlation, defined as

\begin{equation*}R_{ij}(\tau) = E[n_i(t) m_j(t+\tau)]\end{equation*}

and the covariance, 

\begin{equation*}\mbox{Cov}_{ij}(\tau) = R_{ij}(t) - p_np_m\end{equation*}

If $\int$Cov$_{ij}(\tau) d\tau = 0$, then the signals are independent. Finally, the transition probability can be estimated using

\begin{equation*}\alpha_{ij}(\tau) = \mbox{Cov}_{ij}(\tau) / p_n(1-p_n)\end{equation*}

</p>
<a href='#top'>[top]</a>

<a name="3"></a><h2>Implementation</h2>
<p>The entry/exit zones are known before starting Network Calibration. They can be leant using Expectation-Maximization on a dataset of trajectories as did Makris et al for their implementation, which is presented in one of their preious works. <a href="#r3">[3]</a> Let $Z$ be the set of learnt zones. Each signal $N_i$ from some zone $i\in Z$ is modeled as a Guassian distribution with mean $\boldsymbol\mu_i$, covariance $\boldsymbol\Sigma_i$ and a prior probability $p_i = p_n$. Together, the network is represented by a Gaussian Mixture $\theta = \{\boldsymbol\mu_i, \boldsymbol\Sigma_i, p_i | i\in Z\}$. </p>
<p>Further observations of trajectories are done by the cameras to learn the topology. This process is mainly divided into three parts: data classification, parameter estimations, interpretation. </p>

<a name="33"></a><h3>Data Classification</h3>
<p>For each signal $x$ related to an appearance/disappearance of a target, Maximum-a-Posteriori is used to estimate which entry/exit zone $x$ belongs to using the following equations.

\begin{equation*}\mbox{most likely zone = arg}\max_{i\in Z} (p(i|x,\theta))\end{equation*}

where

\begin{align}
	&p(i|x,\theta) = \eta p_i p(x|i)\\
	&p(x|i) = \frac{1}{\sqrt{2\pi|\boldsymbol\Sigma_i|}} \exp{\left(-\frac{1}{2}(x - \boldsymbol\mu_i)^\top \boldsymbol\Sigma_i^{-1}(x - \boldsymbol\mu_i)\right)}
\end{align}

Equation (1) follows from Bayes Rule; $\eta = \sum_{j\in Z} p_j p(x|j)$ is the normalization factor. Equation (2) decribes the probability of having a signal at zone $i$, which is assumed to follow the guassian distribution. The logarithm of the probabilities is used instead and the equations can be simplified into the following.

\begin{equation*} p(i|x,\theta) \propto \log{(p_i p(x|i))} - \frac{1}{2}\log{|\boldsymbol\Sigma_i|} -\frac{1}{2}(x - \boldsymbol\mu_i)^\top \boldsymbol\Sigma_i^{-1}(x - \boldsymbol\mu_i)\end{equation*}
</p> 
<a href='#top'>[top]</a>

<a name="4"></a><h2>Results</h2>

We found.....  illustrated here:
<img src="http://www.cs.mcgill.ca/~me/whatever" alt="radiosity illustrated">

Our results could be expanded as follows....
Other data that could be collected is...

<a name="5"></a><h2>Conclusions </h2>
In summary, our results indicates that....
The advantages of the method are....
The disadvantages of the method include....
Other possible avenues for exploration include....

<a name="6"></a><h2>References</h2>
<p><a name="r1"></a>[1] Makris, D.; Ellis, T.; Black, J. "Bridging the gaps between cameras",  Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, On page(s): II-205 - II-210 Vol.2 Volume: 2, 27 June-2 July 2004 </p>
<p><a name="r2"></a>[2] Omar Javed, Zeeshan Rasheed, Khurram Shafique, Mubarak Shah, "Tracking Across Multiple Cameras With Disjoint Views", IEEE International Conference on Computer Vision, Nice, France, 2003. </p>
<p><a name="r3"></a>[3] D. Makris, T.J. Ellis, "Automatic Learning of an Activity-Based Semantic Scene Model", IEEE International Conference on Advanced Video and Signal Based Surveillance, Miami, FL, USA, pp. 183-188. July 2003.  </p>

<a href='#top'>[top]</a>
</BODY>
</html>

